install.packages(c("reticulate", "dplyr", "tidyr", "ggplot2", "lubridate"))
library(reticulate)
library(dplyr)
library(lubridate)
datasets <- import("datasets")
reticulate::py_install("datasets", pip = TRUE)
datasets <- import("datasets")
boom <- datasets$load_dataset("Datadog/BOOM")  # pulls public data; no account required
library(reticulate)
# 1) paste your token (you can also set this in .Renviron)
Sys.setenv(HF_TOKEN = "hf_your_personal_token_here")
# 1) paste your token (you can also set this in .Renviron)
Sys.setenv(HF_TOKEN = "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")
# 2) (optional) log in the Python side so 'datasets' reuses the token
hub <- import("huggingface_hub")
hub$login(token = Sys.getenv("HF_TOKEN"))
# 3) Load a small slice to avoid big downloads and stay under limits
datasets <- import("datasets")
boom <- datasets$load_dataset(
"Datadog/BOOM",
split = "train[:1%]",                 # 1% slice; adjust to 0.5%/2% etc.
use_auth_token = Sys.getenv("HF_TOKEN")
)
boom <- datasets$load_dataset(
"Datadog/BOOM",
split = "train[:1%]",     # try "test[:1%]" if there is no train split
token = Sys.getenv("HF_TOKEN")
)
boom <- datasets$load_dataset("Datadog/BOOM", split = "train[:1%]")
# 3) Load a small slice to avoid big downloads and stay under limits
datasets <- import("datasets")
boom <- datasets$load_dataset("Datadog/BOOM", split = "train[:1%]")
datasets <- import("datasets")
# See actual split names first (some datasets only have "test")
print(datasets$get_dataset_split_names("Datadog/BOOM", token = Sys.getenv("HF_TOKEN")))
# Stream rows so you don't cache the whole thing
ds_stream <- datasets$load_dataset(
"Datadog/BOOM",
split    = "train",            # change to "test" if that's what you see above
streaming = TRUE,
token     = Sys.getenv("HF_TOKEN")
)
# Pull ~5k rows into R
py_iter <- import("builtins")$iter
it <- py_iter(ds_stream)
batch <- lapply(1:5000, function(i) reticulate::py_next(it, default = NULL))
datasets <- import("datasets")
# See actual split names first (some datasets only have "test")
print(datasets$get_dataset_split_names("Datadog/BOOM", token = Sys.getenv("HF_TOKEN")))
# Stream rows so you don't cache the whole thing
ds_stream <- datasets$load_dataset(
"Datadog/BOOM",
split    = "train",            # change to "test" if that's what you see above
streaming = TRUE,
token     = Sys.getenv("HF_TOKEN")
)
datasets <- import("datasets")
# Try a very small slice to keep it snappy
boom <- datasets$load_dataset(
"Datadog/BOOM",
split = "train[:0.5%]",        # or "test[:0.5%]" based on splits
token = Sys.getenv("HF_TOKEN"),
cache_dir = normalizePath("~/.cache/hf", mustWork = FALSE)
)
# See actual split names first (some datasets only have "test")
print(datasets$get_dataset_split_names("Datadog/BOOM"))
library(reticulate)
# 1) Put HF cache in a local folder NOT synced by OneDrive
cache <- "C:/hf_cache"   # pick any local path outside OneDrive
dir.create(cache, recursive = TRUE, showWarnings = FALSE)
Sys.setenv(
HF_HOME = cache,
HF_DATASETS_CACHE = file.path(cache, "datasets"),
HF_HUB_DISABLE_SYMLINKS_WARNING = "1"
)
# 2) (One-time) remove broken BOOM cache under OneDrive, if it exists
bad <- file.path(Sys.getenv("USERPROFILE"), "OneDrive", "Documents", ".cache", "hf")
if (dir.exists(bad)) unlink(bad, recursive = TRUE, force = TRUE)
# 3) Login (prevents rate limits) – paste your token
hub <- import("huggingface_hub")
hub$login(token = Sys.getenv("HF_TOKEN"))
library(reticulate)
library(dplyr)
library(lubridate)
datasets <- import("datasets")
# See actual split names first (some datasets only have "test")
print(datasets$get_dataset_split_names("Datadog/BOOM"))
# Try a very small slice to keep it snappy
boom <- datasets$load_dataset(
"Datadog/BOOM",
split = "train[:0.5%]",        # or "test[:0.5%]" based on splits
cache_dir = normalizePath("~/.cache/hf", mustWork = FALSE)
)
hub$login(token = "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")
# 4) Load a tiny slice and cache to the new location
datasets <- import("datasets")
boom <- datasets$load_dataset(
"Datadog/BOOM",
split    = "train[:0.5%]",     # try "test[:0.5%]" if no "train"
token    = "hf_xxx_your_token_here",
cache_dir = cache,
verification_mode = "no_checks"  # skip extra validation on Windows
)
boom <- datasets$load_dataset(
"Datadog/BOOM",
split    = "train[:0.5%]",     # try "test[:0.5%]" if no "train"
cache_dir = cache,
verification_mode = "no_checks"  # skip extra validation on Windows
)
# 0) OPTIONAL: use a stable reticulate env
reticulate::use_virtualenv("r-reticulate", required = TRUE)
library(reticulate)
# 0) OPTIONAL: use a stable reticulate env
reticulate::use_virtualenv("r-reticulate", required = TRUE)
datasets <- import("datasets")
# See actual split names first (some datasets only have "test")
print(datasets$get_dataset_split_names("Datadog/BOOM"))
# If you see only "test", use that instead of "train"
ds_stream <- datasets$load_dataset(
"Datadog/BOOM",
split     = "train",        # or "test"
streaming = TRUE
)
# ---- 3) Pull ~5k rows from the stream ----
builtins <- import_builtins()
it <- builtins$iter(ds_stream)
get_n_rows <- function(it, n = 5000){
rows <- vector("list", n)
k <- 0L
repeat {
if (k >= n) break
# py_next() exists in reticulate; call it in a tryCatch loop
rec <- try(reticulate::py_next(it), silent = TRUE)
if (inherits(rec, "try-error") || is.null(rec)) break
k <- k + 1L
rows[[k]] <- as.data.frame(rec)
}
rows <- rows[seq_len(k)]
if (!length(rows)) stop("Stream returned no rows.")
do.call(rbind.data.frame, rows)
}
raw_df <- get_n_rows(it, n = 5000)
# List available CONFIGS for Datadog/BOOM
cfgs <- datasets$get_dataset_config_names("Datadog/BOOM")
print(cfgs)
# For each config, list its SPLITS (train/test/validation...)
for (nm in cfgs) {
cat("\nCONFIG:", nm, "\n")
print(datasets$get_dataset_split_names("Datadog/BOOM",
config_name = nm,
token = Sys.getenv("HF_TOKEN")))
}
cfg <- "default"
spl <- "train"   # e.g., "train" or "test"
ds_stream <- datasets$load_dataset(
"Datadog/BOOM",
name      = cfg,
split     = spl,
streaming = TRUE
)
# Pull ~5k rows
builtins <- import_builtins()
it <- builtins$iter(ds_stream)
get_n_rows <- function(it, n = 5000){
rows <- vector("list", n)
k <- 0L
repeat {
if (k >= n) break
rec <- try(reticulate::py_next(it), silent = TRUE)
if (inherits(rec, "try-error") || is.null(rec)) break
k <- k + 1L
rows[[k]] <- as.data.frame(rec)
}
rows <- rows[seq_len(k)]
if (!length(rows)) stop("Stream returned no rows (config/split likely empty).")
do.call(rbind.data.frame, rows)
}
raw_df <- get_n_rows(it, n = 5000)
install.packages(c("reticulate","arrow","jsonlite","dplyr","lubridate","ggplot2"))
install.packages(c("reticulate","arrow","jsonlite","dplyr","lubridate","ggplot2"))
install.packages(c("arrow","jsonlite","ggplot2"))
install.packages(c("arrow","jsonlite"))
library(reticulate); library(arrow); library(jsonlite)
library(dplyr); library(lubridate); library(ggplot2)
## 0) Put HF cache somewhere local (not OneDrive) + (optional) login
cache <- "C:/hf_cache"
dir.create(cache, recursive = TRUE, showWarnings = FALSE)
Sys.setenv(HF_HOME = cache, HF_HUB_DISABLE_SYMLINKS_WARNING = "1")
hub <- import("huggingface_hub")
api <- hub$HfApi()
# Optional but helps avoid rate limits:
hub$login(token = "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")
## 1) List repo files and pick a shard we can read directly
files <- api$list_repo_files("Datadog/BOOM")
# List repo files in the DATASET repo
files <- api$list_repo_files("Datadog/BOOM", repo_type = "dataset")
head(files, 30)
# Pick a parquet/jsonl shard (avoid README/taxonomy/metadata)
cand <- files[grepl("\\.(parquet|jsonl(\\.gz)?)$", files, ignore.case = TRUE)]
stopifnot(length(cand) > 0)
# 1) List files from the DATASET repo
files <- api$list_repo_files("Datadog/BOOM", repo_type = "dataset")
# Arrow shards look like: ".../data-00000-of-00001.arrow"
arrow_files <- files[grepl("\\.arrow$", files, ignore.case = TRUE)]
stopifnot(length(arrow_files) > 0)
# 2) Pick one shard (change index if you want a different one)
target <- arrow_files[1]
local_path <- hub$hf_hub_download(
repo_id  = "Datadog/BOOM",
filename = target,
repo_type = "dataset"
# , token = "hf_...your_token..."
)
# 3) Read Arrow IPC to data.frame
tab <- arrow::read_ipc_file(local_path)
# install.packages(c("reticulate","arrow","dplyr","lubridate"))
library(reticulate); library(arrow); library(dplyr); library(lubridate)
# 0) Cache OFF OneDrive
cache <- "C:/hf_cache"
dir.create(cache, recursive = TRUE, showWarnings = FALSE)
Sys.setenv(HF_HOME = cache, HF_HUB_DISABLE_SYMLINKS_WARNING = "1")
# 2) Snapshot only ARROW shards from the DATASET repo (no metadata)
dl_dir <- hub$snapshot_download(
repo_id    = "Datadog/BOOM",
repo_type  = "dataset",
allow_patterns = c("**/data-*.arrow"),  # only the real data files
local_dir  = cache,
local_dir_use_symlinks = FALSE
)
reticulate::py_install("hf_xet", pip = TRUE)
# 3) Pick one arrow shard and read it
arrow_files <- list.files(dl_dir, pattern = "data-.*\\.arrow$", recursive = TRUE, full.names = TRUE)
)
# 3) Pick one arrow shard and read it
arrow_files <- list.files(dl_dir, pattern = "data-.*\\.arrow$", recursive = TRUE, full.names = TRUE)
stopifnot(length(arrow_files) > 0)
local_path <- arrow_files[1]
# Sanity check: file shouldn’t be tiny (HTML error pages are small)
stopifnot(file.size(local_path) > 1024)
tab <- arrow::read_ipc_file(local_path)   # use read_ipc_stream(local_path) if this ever complains
# 3) Load a tiny slice and force pandas format (avoids pyarrow/arrow conversion)
#    Use 'train[:0.5%]' or even '[:0.2%]' to keep it small for class.
ds <- datasets$load_dataset(
"Datadog/BOOM",
split = "train[:0.5%]",
token = Sys.getenv("HF_TOKEN"),
cache_dir = cache
)
datasets <- import("datasets")
hub      <- import("huggingface_hub")
# 3) Load a tiny slice and force pandas format (avoids pyarrow/arrow conversion)
#    Use 'train[:0.5%]' or even '[:0.2%]' to keep it small for class.
ds <- datasets$load_dataset(
"Datadog/BOOM",
split = "train[:0.5%]",
token = Sys.getenv("HF_TOKEN"),
cache_dir = cache
)
# 1) make sure python bits exist
reticulate::py_install(c("datasets","huggingface_hub","pandas"), pip = TRUE)
# 2) (optional but recommended) set your token to avoid rate limits
Sys.setenv(HF_TOKEN = "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")
datasets <- import("datasets")
library(reticulate); library(dplyr); library(ggplot2); library(lubridate)
# Python package bridge
datasets <- import("datasets")
library(reticulate); library(dplyr); library(ggplot2); library(lubridate)
# Python package bridge
datasets <- import("datasets")
reticulate::py_config()
reticulate::py_install("datasets", pip = TRUE)
# Python package bridge
datasets <- import("datasets")
# Limit download with [:0.2%] or [:0.5%] to keep it small
boom <- datasets$load_dataset("Datadog/BOOM", split="train[:0.5%]")
reticulate::py_install(c("datasets", "pandas"), pip = TRUE)
# Install reticulate if you don’t already have it
install.packages("reticulate")
# Install HuggingFace datasets via Python (once)
reticulate::py_install("datasets")
install.packages("reticulate")
install.packages("reticulate")
install.packages("reticulate")
library(reticulate)
# 1) Install a private Miniconda just for R (one time)
reticulate::install_miniconda()
app <- read.csv("boom_csv/application.csv")
install.packages("arrow")
library(arrow)
# Pick any downloaded shard:
path <- "C:/boom/boom_arrow/ds-0-T__data-00000-of-00001.arrow"
tab <- read_arrow(path)         # arrow::read_arrow handles .arrow IPC files
library(arrow)
# Point to the Arrow shard you downloaded
path <- "C:/boom/boom_arrow/ds-0-T__data-00000-of-00001.arrow"
# Read as Arrow IPC / Feather
tab <- read_ipc_file(path)
# install.packages(c("reticulate", "readr", "dplyr"))
library(reticulate)
datasets <- import("datasets")
reticulate::py_install("datasets", pip = TRUE)
datasets <- import("datasets")
# ---- Packages ----
req <- c("readr","dplyr","tsibble","fable","fabletools","feasts","lubridate","forecast")
new <- setdiff(req, rownames(installed.packages()))
if (length(new)) install.packages(new, repos = "https://cloud.r-project.org")
invisible(lapply(req, library, character.only = TRUE))
# ---- Choose subset(s) ----
urls <- list(
ETTh1 = "https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTh1.csv",
ETTm1 = "https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm1.csv"
)
# ---- Helper to load one ETT CSV into a tsibble ----
load_ett <- function(name, url) {
message("Downloading: ", name)
df <- readr::read_csv(url, show_col_types = FALSE, progress = FALSE)
# Ensure expected columns exist; coerce date/time
stopifnot(all(c("date","HUFL","HULL","MUFL","MULL","LUFL","LULL","OT") %in% names(df)))
df <- df %>%
mutate(
date = ymd_hms(date, quiet = TRUE) %||% ymd_hm(date, quiet = TRUE) %||% ymd(date)
)
# Build a single-key tsibble (one device/transformer stream)
df_ts <- df %>%
as_tsibble(index = date)
# Add frequency metadata for convenience
attr(df_ts, "ett_name") <- name
df_ts
}
`%||%` <- function(a,b) if (!all(is.na(a))) a else b
ett_h <- load_ett("ETTh1", urls$ETTh1)   # hourly
ett_m <- load_ett("ETTm1", urls$ETTm1)   # 15-min
setwd("~/GitHub/strall.com/DSC425-Final-Project")
